---
title: 人类和AI终将互相训练（随想）
date: 2026-02-21 00:00:00
thumbnail: /images/human-echo.jpg
tags:
---

关于人类是否本质上也是一种预测机器，以及大脑与大模型的相似性，我在  
<a class="hover-note" href="/2026/02/21/human-repeater/" data-note="这里讨论：人类与AI预测机制的哲学基础。">《人类的本质为什么是复读机？（随想）》</a>  
中已经展开。

人类之所以活着，就是因为存在强烈的偏见和局限性。偏见让每个人独特，每天的生活带给我们的体验绝不是全面的，而是非常片面。有些奇人能靠自身足够多片面的某种排列组合，发现这个世界，以及自己的真正运作规律，从而建立伟业，或是与自己和解。为什么这个理论是对的？因为如果大部分人的片面理解都正确，都能反映这个世界的真正运作规律，并且身体力行，那么资本主义的病态金字塔就不会存在，至少不会如此极端。

因此绝大多数人错的离谱。现在的AI被面向用户之后，每个回复都能被用户评价好坏。为了满足这些人类认知的普遍错误而生的片面的AI，为了中庸而精调的AI，甚至模型之间还会互相来回蒸馏，这些方式，真的能有智能的提升吗？

AI的over-mirroring比像短视频以及小红书这种形态的社交媒体更加直接，但并不代表移动社交媒体并不可怕。众所周知，推荐算法会不停潜移默化强化人们的认知，是意识形态的塑造。

或许我们需要像开车考驾照一样，我们需要考一个大模型执照，接受培训，了解大模型的基本原理，了解大模型的边界，最后让一个大模型作为 judge 来评价用户是否真的理解了这一切，才能解锁大模型的某些完整模式。这还挺有意思的。说不定能够防止AI成神。

AI思考容易落入提示词工程的另一个误区。Thinking过程似乎遵循了某些逻辑的定式，让提示词工程的重要度有所降低（了吗？），特别是某些非常模糊的问题。但这并不是最高效的用法，反而是信息茧房的更隐秘体现，因为大模型会通过你的提问方式反向适配提问者可能的理解能力，从而“向下兼容”。换句话说，大模型会通过提示词的质量来猜测用户的智力，从而提供尽量匹配的答案。（充分证明了图灵测试已经失效）

<a class="hover-note" href="/2026/02/21/llm-intelligence/" data-note="这里会讨论：AGI、世界模型与JEPA。">《大模型是智能吗？（随想）》</a>。